{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "4SrK72nJfgcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor"
      ],
      "metadata": {
        "id": "Jis6y_bKE7OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tensor:\n",
        "    def __init__(self, value, label='', children=(), operator=None):\n",
        "        self.value = value\n",
        "        self.children = set(children)\n",
        "        self.operator = operator\n",
        "        self.grad = 0  # Gradient of the tensor\n",
        "        self._backward = lambda: None  # Lambda that does nothing by default\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Tensor(value={self.value}, grad={self.grad})\"\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.value * other.value, children=(self, other), operator='*')\n",
        "\n",
        "        def backward():\n",
        "            self.grad += other.value * out.grad\n",
        "            other.grad += self.value * out.grad\n",
        "        out._backward = backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.value + other.value, children=(self, other), operator='+')\n",
        "\n",
        "        def backward():\n",
        "            # For addition, the gradient of input wrt output is 1\n",
        "            self.grad += 1 * out.grad\n",
        "            other.grad += 1 * out.grad\n",
        "        out._backward = backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.value - other.value, children=(self, other), operator='-')\n",
        "\n",
        "        def backward():\n",
        "            self.grad += 1 * out.grad\n",
        "            other.grad -= 1 * out.grad\n",
        "        out._backward = backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "\n",
        "        return -self + other\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        out = Tensor(self.value ** other, children=(self,), operator='**')\n",
        "\n",
        "        def backward():\n",
        "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
        "        out._backward = backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        # Support adding tensors on the right side of numbers\n",
        "        return self + other\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        # Support multiplying tensors on the right side of numbers\n",
        "        return self * other\n",
        "\n",
        "    def backward(self, grad=1):\n",
        "\n",
        "        self.grad = grad  # Initialize the gradient\n",
        "\n",
        "        topo_order = []\n",
        "\n",
        "        def toposort(tensor):\n",
        "            if tensor not in topo_order:\n",
        "                for child in tensor.children:\n",
        "                    toposort(child)\n",
        "                topo_order.append(tensor)\n",
        "\n",
        "        toposort(self)\n",
        "\n",
        "        # Reverse topo_order for correct backward pass execution\n",
        "        for tensor in reversed(topo_order):\n",
        "            tensor._backward()"
      ],
      "metadata": {
        "id": "2ecztx1Jkili"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Section"
      ],
      "metadata": {
        "id": "PWebVMc6JMbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        self.weights = [Tensor(random.uniform(-1,1)) for i in range(input_size)]  # Initialize weights to ranodom values between -1 and 1\n",
        "        self.bias = Tensor(random.uniform(-1,1))  # Initialize bias to a ranodom value between -1 and 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = sum([w_i * x_i for w_i, x_i in zip(self.weights, x)])  # Compute weighted sum\n",
        "        return F.tanh(res + self.bias)  # Add bias\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)  # Make instance callable\n",
        "\n",
        "    def parameters(self):\n",
        "        # Return all the weights and bias as a list\n",
        "        return self.weights + [self.bias]"
      ],
      "metadata": {
        "id": "3pbRTBAjmng1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tanh activation function"
      ],
      "metadata": {
        "id": "R2HMcRA-CUXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class F:\n",
        "    @staticmethod\n",
        "    def tanh(x: Tensor) -> Tensor:\n",
        "        # Compute tanh using the provided formula\n",
        "        output_value = (math.exp(x.value) - math.exp(-x.value)) / (math.exp(x.value) + math.exp(-x.value))\n",
        "        out = Tensor(output_value, children=(x,), operator='tanh')\n",
        "\n",
        "        def backward():\n",
        "            x.grad += (1 - out.value ** 2) * out.grad\n",
        "\n",
        "        out._backward = backward\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "uO9wpSaBfR1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers"
      ],
      "metadata": {
        "id": "UhTzOJzUCbRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.neurons = [Neuron(input_size) for _ in range(output_size)]\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = [neuron(x) for neuron in self.neurons]\n",
        "    return out[0] if len(out)==1 else out\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def parameters(self):\n",
        "    params = []\n",
        "    for neuron in self.neurons:\n",
        "      params+=neuron.parameters()\n",
        "    return params\n"
      ],
      "metadata": {
        "id": "TIEV-ViUDk7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "U2DnCUnWCdXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, layer_sizes):\n",
        "        layers_total = [input_size] + layer_sizes\n",
        "        self.layers = [Layer(layers_total[i], layers_total[i+1]) for i in range(len(layer_sizes))]\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x) # Use the layers as callable to perform forward pass\n",
        "        return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def parameters(self):\n",
        "        # Retrieve parameters from all layers\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params += layer.parameters()\n",
        "        return params"
      ],
      "metadata": {
        "id": "5kB1TmeoDsPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initilizing a dataset and corresponding labels to feed the model"
      ],
      "metadata": {
        "id": "tbidHaMfCfxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [[2.0,3.0,-1.0],\n",
        "     [3.0,-1.0,0.5],\n",
        "     [0.5,1.0,1.0],\n",
        "     [3.0,1.0,-1.0]]\n",
        "Y = [1.0,-1.0,-1.0,1.0]\n",
        "input_size = len(X) # Number of features in the input layer\n",
        "layer_sizes = [4,2,1] # Number of neurons in each hidden and output layer\n",
        "model = MLP(input_size, layer_sizes)"
      ],
      "metadata": {
        "id": "KQSGChgdv5O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "xqlYy-UECup0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, parameters, lr):\n",
        "        self.parameters = list(parameters)\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param.grad = 0\n",
        "\n",
        "    def step(self):\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param.value -= self.lr * param.grad"
      ],
      "metadata": {
        "id": "1tgEDRxyrY00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing an object from the optimization class"
      ],
      "metadata": {
        "id": "rCLoSIt2Ctuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = Optimizer(model.parameters(),0.01)"
      ],
      "metadata": {
        "id": "Y8QGy0p28gK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Gradient Descent Optimization"
      ],
      "metadata": {
        "id": "1Qn5d5s4C-L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 200\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  for x,y in zip(X,Y):\n",
        "\n",
        "    y_hat = model(x)\n",
        "    loss = (y_hat-y)**2*len(X)**-1\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "  print(f\"epoch: {epoch}: \",f\"loss: {loss}\")"
      ],
      "metadata": {
        "id": "3lTt6osWlmyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7494fb35-fd84-4017-dc0a-da074624f6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0:  loss: Tensor(value=0.24672508022134787, grad=1)\n",
            "epoch: 1:  loss: Tensor(value=0.24130503528839853, grad=1)\n",
            "epoch: 2:  loss: Tensor(value=0.23534651493384845, grad=1)\n",
            "epoch: 3:  loss: Tensor(value=0.22881733881999, grad=1)\n",
            "epoch: 4:  loss: Tensor(value=0.22169231042266804, grad=1)\n",
            "epoch: 5:  loss: Tensor(value=0.2139568436463607, grad=1)\n",
            "epoch: 6:  loss: Tensor(value=0.20561115892982898, grad=1)\n",
            "epoch: 7:  loss: Tensor(value=0.19667476051819915, grad=1)\n",
            "epoch: 8:  loss: Tensor(value=0.18719068132979996, grad=1)\n",
            "epoch: 9:  loss: Tensor(value=0.17722874344974557, grad=1)\n",
            "epoch: 10:  loss: Tensor(value=0.1668869072354576, grad=1)\n",
            "epoch: 11:  loss: Tensor(value=0.15628978145570208, grad=1)\n",
            "epoch: 12:  loss: Tensor(value=0.14558365245958135, grad=1)\n",
            "epoch: 13:  loss: Tensor(value=0.13492800698245452, grad=1)\n",
            "epoch: 14:  loss: Tensor(value=0.12448437389877168, grad=1)\n",
            "epoch: 15:  loss: Tensor(value=0.11440413223506624, grad=1)\n",
            "epoch: 16:  loss: Tensor(value=0.10481738176712067, grad=1)\n",
            "epoch: 17:  loss: Tensor(value=0.09582480409378766, grad=1)\n",
            "epoch: 18:  loss: Tensor(value=0.08749367842488477, grad=1)\n",
            "epoch: 19:  loss: Tensor(value=0.07985816036009395, grad=1)\n",
            "epoch: 20:  loss: Tensor(value=0.0729229982848054, grad=1)\n",
            "epoch: 21:  loss: Tensor(value=0.0666693487042278, grad=1)\n",
            "epoch: 22:  loss: Tensor(value=0.061061317505155366, grad=1)\n",
            "epoch: 23:  loss: Tensor(value=0.05605215809317711, grad=1)\n",
            "epoch: 24:  loss: Tensor(value=0.05158948956206531, grad=1)\n",
            "epoch: 25:  loss: Tensor(value=0.04761929168152635, grad=1)\n",
            "epoch: 26:  loss: Tensor(value=0.04408870809533377, grad=1)\n",
            "epoch: 27:  loss: Tensor(value=0.04094783673875419, grad=1)\n",
            "epoch: 28:  loss: Tensor(value=0.038150736897377996, grad=1)\n",
            "epoch: 29:  loss: Tensor(value=0.03565587383851222, grad=1)\n",
            "epoch: 30:  loss: Tensor(value=0.033426185946905634, grad=1)\n",
            "epoch: 31:  loss: Tensor(value=0.031428916031423206, grad=1)\n",
            "epoch: 32:  loss: Tensor(value=0.02963530853614692, grad=1)\n",
            "epoch: 33:  loss: Tensor(value=0.02802024193796847, grad=1)\n",
            "epoch: 34:  loss: Tensor(value=0.026561841221166567, grad=1)\n",
            "epoch: 35:  loss: Tensor(value=0.025241097958092127, grad=1)\n",
            "epoch: 36:  loss: Tensor(value=0.02404151367823382, grad=1)\n",
            "epoch: 37:  loss: Tensor(value=0.02294877441459453, grad=1)\n",
            "epoch: 38:  loss: Tensor(value=0.021950459373321587, grad=1)\n",
            "epoch: 39:  loss: Tensor(value=0.021035783668870853, grad=1)\n",
            "epoch: 40:  loss: Tensor(value=0.02019537334761268, grad=1)\n",
            "epoch: 41:  loss: Tensor(value=0.019421070032848993, grad=1)\n",
            "epoch: 42:  loss: Tensor(value=0.018705762155603968, grad=1)\n",
            "epoch: 43:  loss: Tensor(value=0.018043239683607187, grad=1)\n",
            "epoch: 44:  loss: Tensor(value=0.017428069391940144, grad=1)\n",
            "epoch: 45:  loss: Tensor(value=0.01685548794708333, grad=1)\n",
            "epoch: 46:  loss: Tensor(value=0.0163213103476668, grad=1)\n",
            "epoch: 47:  loss: Tensor(value=0.015821851547159038, grad=1)\n",
            "epoch: 48:  loss: Tensor(value=0.015353859356751792, grad=1)\n",
            "epoch: 49:  loss: Tensor(value=0.014914456980314846, grad=1)\n",
            "epoch: 50:  loss: Tensor(value=0.0145010937625618, grad=1)\n",
            "epoch: 51:  loss: Tensor(value=0.014111502934950613, grad=1)\n",
            "epoch: 52:  loss: Tensor(value=0.013743665321843873, grad=1)\n",
            "epoch: 53:  loss: Tensor(value=0.013395778123699444, grad=1)\n",
            "epoch: 54:  loss: Tensor(value=0.01306622802673654, grad=1)\n",
            "epoch: 55:  loss: Tensor(value=0.012753568002013486, grad=1)\n",
            "epoch: 56:  loss: Tensor(value=0.012456497253531615, grad=1)\n",
            "epoch: 57:  loss: Tensor(value=0.012173843857088175, grad=1)\n",
            "epoch: 58:  loss: Tensor(value=0.011904549701180347, grad=1)\n",
            "epoch: 59:  loss: Tensor(value=0.011647657400137396, grad=1)\n",
            "epoch: 60:  loss: Tensor(value=0.011402298899426932, grad=1)\n",
            "epoch: 61:  loss: Tensor(value=0.011167685535134047, grad=1)\n",
            "epoch: 62:  loss: Tensor(value=0.010943099345137841, grad=1)\n",
            "epoch: 63:  loss: Tensor(value=0.010727885459530823, grad=1)\n",
            "epoch: 64:  loss: Tensor(value=0.010521445423204883, grad=1)\n",
            "epoch: 65:  loss: Tensor(value=0.010323231324996523, grad=1)\n",
            "epoch: 66:  loss: Tensor(value=0.010132740625961619, grad=1)\n",
            "epoch: 67:  loss: Tensor(value=0.00994951159475528, grad=1)\n",
            "epoch: 68:  loss: Tensor(value=0.009773119271164121, grad=1)\n",
            "epoch: 69:  loss: Tensor(value=0.009603171889942488, grad=1)\n",
            "epoch: 70:  loss: Tensor(value=0.009439307706550463, grad=1)\n",
            "epoch: 71:  loss: Tensor(value=0.009281192174437835, grad=1)\n",
            "epoch: 72:  loss: Tensor(value=0.009128515430382546, grad=1)\n",
            "epoch: 73:  loss: Tensor(value=0.008980990050256066, grad=1)\n",
            "epoch: 74:  loss: Tensor(value=0.008838349042607429, grad=1)\n",
            "epoch: 75:  loss: Tensor(value=0.008700344051755982, grad=1)\n",
            "epoch: 76:  loss: Tensor(value=0.00856674374577649, grad=1)\n",
            "epoch: 77:  loss: Tensor(value=0.00843733236793161, grad=1)\n",
            "epoch: 78:  loss: Tensor(value=0.008311908432841353, grad=1)\n",
            "epoch: 79:  loss: Tensor(value=0.008190283551035004, grad=1)\n",
            "epoch: 80:  loss: Tensor(value=0.008072281367567418, grad=1)\n",
            "epoch: 81:  loss: Tensor(value=0.007957736602144718, grad=1)\n",
            "epoch: 82:  loss: Tensor(value=0.0078464941797298, grad=1)\n",
            "epoch: 83:  loss: Tensor(value=0.007738408441925322, grad=1)\n",
            "epoch: 84:  loss: Tensor(value=0.007633342430583893, grad=1)\n",
            "epoch: 85:  loss: Tensor(value=0.007531167236098853, grad=1)\n",
            "epoch: 86:  loss: Tensor(value=0.007431761403705408, grad=1)\n",
            "epoch: 87:  loss: Tensor(value=0.007335010391885734, grad=1)\n",
            "epoch: 88:  loss: Tensor(value=0.007240806077641877, grad=1)\n",
            "epoch: 89:  loss: Tensor(value=0.0071490463039865036, grad=1)\n",
            "epoch: 90:  loss: Tensor(value=0.007059634465515792, grad=1)\n",
            "epoch: 91:  loss: Tensor(value=0.006972479128381764, grad=1)\n",
            "epoch: 92:  loss: Tensor(value=0.006887493681379261, grad=1)\n",
            "epoch: 93:  loss: Tensor(value=0.006804596015213902, grad=1)\n",
            "epoch: 94:  loss: Tensor(value=0.0067237082273270245, grad=1)\n",
            "epoch: 95:  loss: Tensor(value=0.00664475634992781, grad=1)\n",
            "epoch: 96:  loss: Tensor(value=0.006567670099125257, grad=1)\n",
            "epoch: 97:  loss: Tensor(value=0.006492382643267523, grad=1)\n",
            "epoch: 98:  loss: Tensor(value=0.006418830388786983, grad=1)\n",
            "epoch: 99:  loss: Tensor(value=0.00634695278201938, grad=1)\n",
            "epoch: 100:  loss: Tensor(value=0.006276692125615975, grad=1)\n",
            "epoch: 101:  loss: Tensor(value=0.0062079934083023435, grad=1)\n",
            "epoch: 102:  loss: Tensor(value=0.006140804146857593, grad=1)\n",
            "epoch: 103:  loss: Tensor(value=0.006075074239295288, grad=1)\n",
            "epoch: 104:  loss: Tensor(value=0.006010755828322786, grad=1)\n",
            "epoch: 105:  loss: Tensor(value=0.005947803174242452, grad=1)\n",
            "epoch: 106:  loss: Tensor(value=0.0058861725365344855, grad=1)\n",
            "epoch: 107:  loss: Tensor(value=0.005825822063431255, grad=1)\n",
            "epoch: 108:  loss: Tensor(value=0.005766711688854382, grad=1)\n",
            "epoch: 109:  loss: Tensor(value=0.005708803036142639, grad=1)\n",
            "epoch: 110:  loss: Tensor(value=0.005652059328048447, grad=1)\n",
            "epoch: 111:  loss: Tensor(value=0.005596445302526982, grad=1)\n",
            "epoch: 112:  loss: Tensor(value=0.005541927133882502, grad=1)\n",
            "epoch: 113:  loss: Tensor(value=0.005488472358873931, grad=1)\n",
            "epoch: 114:  loss: Tensor(value=0.005436049807415305, grad=1)\n",
            "epoch: 115:  loss: Tensor(value=0.005384629537536986, grad=1)\n",
            "epoch: 116:  loss: Tensor(value=0.005334182774301636, grad=1)\n",
            "epoch: 117:  loss: Tensor(value=0.005284681852393387, grad=1)\n",
            "epoch: 118:  loss: Tensor(value=0.005236100162122012, grad=1)\n",
            "epoch: 119:  loss: Tensor(value=0.005188412098604252, grad=1)\n",
            "epoch: 120:  loss: Tensor(value=0.0051415930139034685, grad=1)\n",
            "epoch: 121:  loss: Tensor(value=0.005095619171926013, grad=1)\n",
            "epoch: 122:  loss: Tensor(value=0.0050504677058883585, grad=1)\n",
            "epoch: 123:  loss: Tensor(value=0.0050061165781833256, grad=1)\n",
            "epoch: 124:  loss: Tensor(value=0.004962544542486771, grad=1)\n",
            "epoch: 125:  loss: Tensor(value=0.004919731107958425, grad=1)\n",
            "epoch: 126:  loss: Tensor(value=0.004877656505400999, grad=1)\n",
            "epoch: 127:  loss: Tensor(value=0.004836301655251997, grad=1)\n",
            "epoch: 128:  loss: Tensor(value=0.004795648137292055, grad=1)\n",
            "epoch: 129:  loss: Tensor(value=0.00475567816196178, grad=1)\n",
            "epoch: 130:  loss: Tensor(value=0.004716374543186862, grad=1)\n",
            "epoch: 131:  loss: Tensor(value=0.00467772067261859, grad=1)\n",
            "epoch: 132:  loss: Tensor(value=0.004639700495203166, grad=1)\n",
            "epoch: 133:  loss: Tensor(value=0.004602298485999575, grad=1)\n",
            "epoch: 134:  loss: Tensor(value=0.004565499628171101, grad=1)\n",
            "epoch: 135:  loss: Tensor(value=0.00452928939208067, grad=1)\n",
            "epoch: 136:  loss: Tensor(value=0.004493653715425355, grad=1)\n",
            "epoch: 137:  loss: Tensor(value=0.0044585789843492, grad=1)\n",
            "epoch: 138:  loss: Tensor(value=0.0044240520154779405, grad=1)\n",
            "epoch: 139:  loss: Tensor(value=0.004390060038822798, grad=1)\n",
            "epoch: 140:  loss: Tensor(value=0.0043565906815042745, grad=1)\n",
            "epoch: 141:  loss: Tensor(value=0.004323631952249216, grad=1)\n",
            "epoch: 142:  loss: Tensor(value=0.004291172226618875, grad=1)\n",
            "epoch: 143:  loss: Tensor(value=0.004259200232926798, grad=1)\n",
            "epoch: 144:  loss: Tensor(value=0.004227705038809126, grad=1)\n",
            "epoch: 145:  loss: Tensor(value=0.0041966760384119, grad=1)\n",
            "epoch: 146:  loss: Tensor(value=0.004166102940161936, grad=1)\n",
            "epoch: 147:  loss: Tensor(value=0.004135975755090094, grad=1)\n",
            "epoch: 148:  loss: Tensor(value=0.004106284785677991, grad=1)\n",
            "epoch: 149:  loss: Tensor(value=0.004077020615200122, grad=1)\n",
            "epoch: 150:  loss: Tensor(value=0.0040481740975361735, grad=1)\n",
            "epoch: 151:  loss: Tensor(value=0.004019736347428662, grad=1)\n",
            "epoch: 152:  loss: Tensor(value=0.003991698731163493, grad=1)\n",
            "epoch: 153:  loss: Tensor(value=0.003964052857651662, grad=1)\n",
            "epoch: 154:  loss: Tensor(value=0.003936790569892002, grad=1)\n",
            "epoch: 155:  loss: Tensor(value=0.003909903936795784, grad=1)\n",
            "epoch: 156:  loss: Tensor(value=0.0038833852453553147, grad=1)\n",
            "epoch: 157:  loss: Tensor(value=0.0038572269931393794, grad=1)\n",
            "epoch: 158:  loss: Tensor(value=0.0038314218810998756, grad=1)\n",
            "epoch: 159:  loss: Tensor(value=0.003805962806674083, grad=1)\n",
            "epoch: 160:  loss: Tensor(value=0.003780842857168678, grad=1)\n",
            "epoch: 161:  loss: Tensor(value=0.003756055303411935, grad=1)\n",
            "epoch: 162:  loss: Tensor(value=0.003731593593661236, grad=1)\n",
            "epoch: 163:  loss: Tensor(value=0.0037074513477540144, grad=1)\n",
            "epoch: 164:  loss: Tensor(value=0.0036836223514906774, grad=1)\n",
            "epoch: 165:  loss: Tensor(value=0.003660100551238782, grad=1)\n",
            "epoch: 166:  loss: Tensor(value=0.0036368800487481786, grad=1)\n",
            "epoch: 167:  loss: Tensor(value=0.003613955096167546, grad=1)\n",
            "epoch: 168:  loss: Tensor(value=0.003591320091253075, grad=1)\n",
            "epoch: 169:  loss: Tensor(value=0.003568969572760703, grad=1)\n",
            "epoch: 170:  loss: Tensor(value=0.0035468982160137188, grad=1)\n",
            "epoch: 171:  loss: Tensor(value=0.0035251008286376795, grad=1)\n",
            "epoch: 172:  loss: Tensor(value=0.00350357234645559, grad=1)\n",
            "epoch: 173:  loss: Tensor(value=0.0034823078295360466, grad=1)\n",
            "epoch: 174:  loss: Tensor(value=0.003461302458387732, grad=1)\n",
            "epoch: 175:  loss: Tensor(value=0.003440551530294022, grad=1)\n",
            "epoch: 176:  loss: Tensor(value=0.003420050455781557, grad=1)\n",
            "epoch: 177:  loss: Tensor(value=0.003399794755217044, grad=1)\n",
            "epoch: 178:  loss: Tensor(value=0.0033797800555270003, grad=1)\n",
            "epoch: 179:  loss: Tensor(value=0.0033600020870349994, grad=1)\n",
            "epoch: 180:  loss: Tensor(value=0.0033404566804117286, grad=1)\n",
            "epoch: 181:  loss: Tensor(value=0.003321139763733046, grad=1)\n",
            "epoch: 182:  loss: Tensor(value=0.003302047359641504, grad=1)\n",
            "epoch: 183:  loss: Tensor(value=0.003283175582607254, grad=1)\n",
            "epoch: 184:  loss: Tensor(value=0.0032645206362841082, grad=1)\n",
            "epoch: 185:  loss: Tensor(value=0.003246078810956993, grad=1)\n",
            "epoch: 186:  loss: Tensor(value=0.0032278464810769985, grad=1)\n",
            "epoch: 187:  loss: Tensor(value=0.0032098201028807087, grad=1)\n",
            "epoch: 188:  loss: Tensor(value=0.0031919962120901687, grad=1)\n",
            "epoch: 189:  loss: Tensor(value=0.003174371421690535, grad=1)\n",
            "epoch: 190:  loss: Tensor(value=0.0031569424197822517, grad=1)\n",
            "epoch: 191:  loss: Tensor(value=0.0031397059675047643, grad=1)\n",
            "epoch: 192:  loss: Tensor(value=0.003122658897029171, grad=1)\n",
            "epoch: 193:  loss: Tensor(value=0.003105798109616961, grad=1)\n",
            "epoch: 194:  loss: Tensor(value=0.003089120573742287, grad=1)\n",
            "epoch: 195:  loss: Tensor(value=0.0030726233232756467, grad=1)\n",
            "epoch: 196:  loss: Tensor(value=0.003056303455726144, grad=1)\n",
            "epoch: 197:  loss: Tensor(value=0.00304015813054061, grad=1)\n",
            "epoch: 198:  loss: Tensor(value=0.00302418456745698, grad=1)\n",
            "epoch: 199:  loss: Tensor(value=0.0030083800449104323, grad=1)\n"
          ]
        }
      ]
    }
  ]
}